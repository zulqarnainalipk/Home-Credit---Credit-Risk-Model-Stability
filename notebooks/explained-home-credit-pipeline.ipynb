{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156e99f3",
   "metadata": {
    "papermill": {
     "duration": 0.014517,
     "end_time": "2024-05-18T17:50:24.503094",
     "exception": false,
     "start_time": "2024-05-18T17:50:24.488577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ  Home Credit  Risk Model Pipeline\n",
    "\n",
    "Welcome to our advanced pipeline for building and assessing credit risk models using machine learning! ðŸ“ŠðŸ’³\n",
    "\n",
    "## Overview ðŸ“\n",
    "Discover a comprehensive approach to constructing credit risk models. We employ various machine learning algorithms like LightGBM and CatBoost, alongside ensemble techniques for robust predictions. Our pipeline emphasizes data integrity, feature relevance, and model stability, crucial elements in credit risk assessment. ðŸ› ï¸ðŸ’¼\n",
    "\n",
    "## Features ðŸš€\n",
    "- **Data Preprocessing**: Begin with cleaning data, handling missing values, and optimizing memory usage for efficient computation.\n",
    "- **Feature Engineering**: Extract meaningful insights from data using advanced techniques, enhancing model predictive power.\n",
    "- **Model Training**: Train multiple machine learning models such as LightGBM and CatBoost to capture complex relationships and patterns.\n",
    "- **Ensemble Learning**: Combine predictions from various models using our custom Voting Model to achieve higher accuracy and stability. ðŸ¤ðŸ“ˆ\n",
    "\n",
    "**ðŸŒŸ Explore my profile and other public projects, and don't forget to share your feedback!**\n",
    "\n",
    "## ðŸ‘‰ [Visit my Profile](https://www.kaggle.com/code/zulqarnainalipk) ðŸ‘ˆ\n",
    "\n",
    "## Requirements ðŸ› ï¸\n",
    "Ensure you have:\n",
    "- Python 3.7+\n",
    "- Libraries: NumPy, pandas, polars, seaborn, matplotlib, scikit-learn, lightgbm, imbalanced-learn, joblib, catboost\n",
    "\n",
    "## Usage ðŸš€\n",
    "Follow these steps:\n",
    "1. **Data Loading**: Ensure required datasets are available in the specified directory (`/kaggle/input/home-credit-credit-risk-model-stability`).\n",
    "2. **Initialization**: Run initialization code to set up necessary functions and configurations.\n",
    "3. **Data Preprocessing**: Execute data preprocessing steps to handle missing values and optimize memory usage.\n",
    "4. **Feature Engineering**: Use provided feature engineering functions to extract relevant features from the dataset.\n",
    "5. **Model Training**: Train machine learning models like LightGBM and CatBoost using preprocessed data.\n",
    "6. **Ensemble Learning**: Combine predictions from multiple models using the custom Voting Model for improved performance.\n",
    "7. **Evaluation**: Assess ensemble model performance and generate submission files for further analysis.\n",
    "\n",
    "## Note ðŸ“Œ\n",
    "- **Customization**: Feel free to customize the pipeline by adding or modifying features, adjusting model parameters, or experimenting with different algorithms.\n",
    "- **Resource Management**: Monitor memory usage and computational resources, especially during data preprocessing and model training, for smooth execution.\n",
    "\n",
    "## Acknowledgments ðŸ™\n",
    "We acknowledge The Home Credit Group organizers for providing the dataset and the competition platform.\n",
    "\n",
    "Let's dive in! Feel free to reach out if you have any questions or need assistance along the way. ðŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainalipk) ðŸ‘ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e86647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:24.532113Z",
     "iopub.status.busy": "2024-05-18T17:50:24.531784Z",
     "iopub.status.idle": "2024-05-18T17:50:31.168860Z",
     "shell.execute_reply": "2024-05-18T17:50:31.168097Z"
    },
    "papermill": {
     "duration": 6.65412,
     "end_time": "2024-05-18T17:50:31.171123",
     "exception": false,
     "start_time": "2024-05-18T17:50:24.517003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys  # System-specific parameters and functions\n",
    "import subprocess  # Spawn new processes, connect to their input/output/error pipes, and obtain their return codes\n",
    "import os  # Operating system dependent functionality\n",
    "import gc  # Garbage Collector interface\n",
    "from pathlib import Path  # Object-oriented filesystem paths\n",
    "from glob import glob  # Unix style pathname pattern expansion\n",
    "\n",
    "import numpy as np  # Fundamental package for scientific computing with Python\n",
    "import pandas as pd  # Powerful data structures for data manipulation and analysis\n",
    "import polars as pl  # Fast DataFrame library implemented in Rust\n",
    "\n",
    "from datetime import datetime  # Basic date and time types\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import matplotlib.pyplot as plt  # MATLAB-like plotting framework\n",
    "\n",
    "import joblib  # Save and load Python objects\n",
    "\n",
    "import warnings  # Warning control\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Base classes for all estimators in scikit-learn\n",
    "from sklearn.metrics import roc_auc_score  # ROC AUC score\n",
    "import lightgbm as lgb  # LightGBM: Gradient boosting framework\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold  # Cross-validation strategies\n",
    "from imblearn.over_sampling import SMOTE  # Oversampling technique for imbalanced datasets\n",
    "from sklearn.preprocessing import OrdinalEncoder  # Encode categorical features as an integer array\n",
    "from sklearn.impute import KNNImputer  # Imputation for completing missing values using k-Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4abe49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.201061Z",
     "iopub.status.busy": "2024-05-18T17:50:31.200554Z",
     "iopub.status.idle": "2024-05-18T17:50:31.204637Z",
     "shell.execute_reply": "2024-05-18T17:50:31.203824Z"
    },
    "papermill": {
     "duration": 0.020653,
     "end_time": "2024-05-18T17:50:31.206417",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.185764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'  # Setting the root directory path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16eb60",
   "metadata": {
    "papermill": {
     "duration": 0.014601,
     "end_time": "2024-05-18T17:50:31.234864",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.220263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ› ï¸ðŸ“Š Pipeline for  Data Preprocessing \n",
    "\n",
    "Let's create a  class named `Pipeline` containing methods to preprocess data using Pandas and Pipelines. \n",
    "**1. `set_table_dtypes(df)`**\n",
    "- This method iterates through each column in the DataFrame (`df`) and converts the data types based on certain conditions.\n",
    "- If the column name is one of [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"], it converts the column to `Int64`.\n",
    "- If the column name is \"date_decision\", it converts the column to `Date`.\n",
    "- If the last character of the column name is \"P\" or \"A\", it converts the column to `Float64`.\n",
    "- If the last character of the column name is \"M\", it converts the column to `String`.\n",
    "- If the last character of the column name is \"D\", it converts the column to `Date`.\n",
    "- Finally, it returns the DataFrame with modified data types.\n",
    "\n",
    "**2. `handle_dates(df)`**\n",
    "- This method aims to handle date columns in the DataFrame.\n",
    "- It iterates through each column, and if the last character of the column name is \"D\", it performs some operations.\n",
    "- It subtracts the date values in the current column from the values in the \"date_decision\" column.\n",
    "- Then it computes the total days between the two dates.\n",
    "- After processing, it drops the \"date_decision\" and \"MONTH\" columns from the DataFrame.\n",
    "- Finally, it returns the modified DataFrame.\n",
    "\n",
    "**3. `filter_cols(df)`**\n",
    "- This method filters out columns based on certain conditions.\n",
    "- It iterates through each column and checks if the column name is not in [\"target\", \"case_id\", \"WEEK_NUM\"] and if the column type is `String`.\n",
    "- If the number of unique values in the column is either 1 or more than 200, it drops that column.\n",
    "- Finally, it returns the filtered DataFrame.\n",
    "\n",
    "### Study Sources\n",
    "- For learning Pandas and data preprocessing: [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- Understanding Pipelines in data preprocessing: [Scikit-Learn Pipeline Documentation](https://scikit-learn.org/stable/modules/compose.html#pipeline)\n",
    "- Data type conversion and manipulation: [Pandas Data Types and Conversion](https://pandas.pydata.org/pandas-docs/version/1.3/user_guide/basics.html#basics-dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af658c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.264001Z",
     "iopub.status.busy": "2024-05-18T17:50:31.263716Z",
     "iopub.status.idle": "2024-05-18T17:50:31.274575Z",
     "shell.execute_reply": "2024-05-18T17:50:31.273882Z"
    },
    "papermill": {
     "duration": 0.027639,
     "end_time": "2024-05-18T17:50:31.276385",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.248746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df):\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112abd38",
   "metadata": {
    "papermill": {
     "duration": 0.013678,
     "end_time": "2024-05-18T17:50:31.304085",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.290407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ” Aggregator for Feature Extraction \n",
    "\n",
    "Let's create a  `Aggregator` class  to aggregate features from a DataFrame. \n",
    "\n",
    "**1. `num_expr(df)`**\n",
    "- This method extracts numerical features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"P\" or \"A\", indicating some numerical measurements.\n",
    "- For each selected column, it creates an expression to compute the maximum value and aliases it accordingly.\n",
    "- Finally, it returns a list of expressions for maximum values of numerical features.\n",
    "\n",
    " **2. `date_expr(df)`**\n",
    "- This method extracts date-related features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"D\", representing date columns.\n",
    "- Similar to `num_expr`, it creates expressions to compute the maximum date value for each selected column and aliases them.\n",
    "- It returns a list of expressions for maximum date values of date features.\n",
    "\n",
    " **3. `str_expr(df)`**\n",
    "- This method extracts string features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"M\", indicating string type columns.\n",
    "- It creates expressions to compute the maximum string value for each selected column and aliases them accordingly.\n",
    "- Returns a list of expressions for maximum string values of string features.\n",
    "\n",
    " **4. `other_expr(df)`**\n",
    "- This method extracts other miscellaneous features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"T\" or \"L\".\n",
    "- Similar to previous methods, it computes the maximum value for each selected column and aliases them.\n",
    "- Returns a list of expressions for maximum values of miscellaneous features.\n",
    "\n",
    " **5. `count_expr(df)`**\n",
    "- This method extracts count-related features from the DataFrame (`df`).\n",
    "- It selects columns containing \"num_group\" in their names.\n",
    "- It computes the maximum value for each selected column and aliases them.\n",
    "- Returns a list of expressions for maximum count values of count features.\n",
    "\n",
    "**6. `get_exprs(df)`**\n",
    "- This method aggregates all the expressions from the previous methods to get a comprehensive list of feature extraction expressions.\n",
    "- It calls all the individual feature extraction methods and concatenates the resulting lists.\n",
    "- Returns a consolidated list of expressions for all types of features.\n",
    "\n",
    "### Study Sources\n",
    "- For learning about feature extraction and aggregation: [Feature Engineering for Machine Learning](https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles/dp/1491953241)\n",
    "- Understanding Pandas DataFrame manipulation: [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- Relational Algebra and Expressions: [Relational Algebra - Wikipedia](https://en.wikipedia.org/wiki/Relational_algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e33c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.332969Z",
     "iopub.status.busy": "2024-05-18T17:50:31.332698Z",
     "iopub.status.idle": "2024-05-18T17:50:31.342695Z",
     "shell.execute_reply": "2024-05-18T17:50:31.341827Z"
    },
    "papermill": {
     "duration": 0.026716,
     "end_time": "2024-05-18T17:50:31.344666",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.317950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "    \n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max\n",
    "    \n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max\n",
    "    \n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max \n",
    "    \n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n",
    "        return  expr_max\n",
    "    \n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993c3da",
   "metadata": {
    "papermill": {
     "duration": 0.013702,
     "end_time": "2024-05-18T17:50:31.372332",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.358630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# File Reading with Data Preprocessing ðŸ“„\n",
    "\n",
    "The function `read_file(path, depth=None)` reads a Parquet file located at the given `path`, performs data preprocessing using the `Pipeline` class, and optionally aggregates features based on the `depth` parameter using the `Aggregator` class. \n",
    "\n",
    "1. `read_file(path, depth=None)`\n",
    "- **Inputs**:\n",
    "  - `path`: Path to the Parquet file.\n",
    "  - `depth`: An optional parameter indicating the depth of feature aggregation. Default is `None`.\n",
    "- **Output**: Returns a processed DataFrame.\n",
    "- **Process**:\n",
    "  - Reads the Parquet file located at the given `path` using `pl.read_parquet(path)`.\n",
    "  - Performs data preprocessing using the `Pipeline` class by applying the `set_table_dtypes` method to ensure proper data types.\n",
    "  - If `depth` is provided and is either 1 or 2:\n",
    "    - It groups the DataFrame by \"case_id\".\n",
    "    - It aggregates features based on the depth using the `Aggregator` class and the `get_exprs` method.\n",
    "  - Returns the processed DataFrame.\n",
    "\n",
    "### Study Sources\n",
    "- For understanding Parquet file format and reading: [Parquet File Format](https://parquet.apache.org/documentation/latest/)\n",
    "- Data preprocessing with Pandas Pipelines: [Pandas Pipe Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html)\n",
    "- Feature aggregation and group-by operations: [Pandas GroupBy Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)\n",
    "- Aggregating features using Pandas: [Pandas Aggregation Documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c2185e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.401181Z",
     "iopub.status.busy": "2024-05-18T17:50:31.400934Z",
     "iopub.status.idle": "2024-05-18T17:50:31.405715Z",
     "shell.execute_reply": "2024-05-18T17:50:31.404950Z"
    },
    "papermill": {
     "duration": 0.021305,
     "end_time": "2024-05-18T17:50:31.407491",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.386186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc451fe4",
   "metadata": {
    "papermill": {
     "duration": 0.013814,
     "end_time": "2024-05-18T17:50:31.435285",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.421471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ“„Reading Multiple Files with Data Preprocessing \n",
    "\n",
    "Let's create a function `read_files(regex_path, depth=None)` that reads multiple Parquet files matching the specified regex pattern, performs data preprocessing using the `Pipeline` class, optionally aggregates features based on the `depth` parameter using the `Aggregator` class, and concatenates the results.\n",
    "**1. `read_files(regex_path, depth=None)`**\n",
    "- **Inputs**:\n",
    "  - `regex_path`: Regular expression pattern for matching file paths.\n",
    "  - `depth`: An optional parameter indicating the depth of feature aggregation. Default is `None`.\n",
    "- **Output**: Returns a concatenated and processed DataFrame.\n",
    "- **Process**:\n",
    "  - Initializes an empty list `chunks` to store processed DataFrames.\n",
    "  - Iterates through each file path matched by the provided regular expression pattern using `glob(str(regex_path))`.\n",
    "    - Reads each Parquet file using `pl.read_parquet(path)`.\n",
    "    - Performs data preprocessing using the `Pipeline` class by applying the `set_table_dtypes` method.\n",
    "    - If `depth` is provided and is either 1 or 2:\n",
    "      - It groups the DataFrame by \"case_id\".\n",
    "      - It aggregates features based on the depth using the `Aggregator` class and the `get_exprs` method.\n",
    "    - Appends the processed DataFrame to the `chunks` list.\n",
    "  - Concatenates all DataFrames in `chunks` vertically using `pl.concat(chunks, how=\"vertical_relaxed\")`.\n",
    "  - Removes duplicate rows based on the \"case_id\" column using `df.unique(subset=[\"case_id\"])`.\n",
    "  - Returns the concatenated and processed DataFrame.\n",
    "\n",
    "### Study Sources\n",
    "- For understanding file path manipulation and regular expressions: [Python Glob Documentation](https://docs.python.org/3/library/glob.html)\n",
    "- Concatenating DataFrames in Pandas: [Pandas Concatenation Documentation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "- Removing duplicate rows in Pandas: [Pandas Drop Duplicates Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7538708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.465088Z",
     "iopub.status.busy": "2024-05-18T17:50:31.464827Z",
     "iopub.status.idle": "2024-05-18T17:50:31.470405Z",
     "shell.execute_reply": "2024-05-18T17:50:31.469635Z"
    },
    "papermill": {
     "duration": 0.022883,
     "end_time": "2024-05-18T17:50:31.472269",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.449386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1283872",
   "metadata": {
    "papermill": {
     "duration": 0.013765,
     "end_time": "2024-05-18T17:50:31.500032",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.486267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ› ï¸ Feature Engineering Function \n",
    "\n",
    "Function `feature_eng(df_base, depth_0, depth_1, depth_2)` performs feature engineering on a base DataFrame (`df_base`) and multiple sets of additional DataFrames (`depth_0`, `depth_1`, `depth_2`). It adds new features, joins additional DataFrames, and handles dates using the `Pipeline` class.\n",
    "\n",
    " **1. `feature_eng(df_base, depth_0, depth_1, depth_2)`**\n",
    "- **Inputs**:\n",
    "  - `df_base`: Base DataFrame on which feature engineering will be performed.\n",
    "  - `depth_0`, `depth_1`, `depth_2`: Lists of DataFrames representing additional features of different depths.\n",
    "- **Output**: Returns the feature-engineered DataFrame.\n",
    "- **Process**:\n",
    "  - Adds new features to the base DataFrame:\n",
    "    - `month_decision`: Extracts the month from the \"date_decision\" column.\n",
    "    - `weekday_decision`: Extracts the weekday from the \"date_decision\" column.\n",
    "  - Iterates through each set of additional DataFrames (`depth_0`, `depth_1`, `depth_2`):\n",
    "    - Joins each DataFrame to the base DataFrame using the \"case_id\" column as the key and left join method.\n",
    "    - Appends a suffix to the column names to distinguish between different sets of features.\n",
    "  - Performs date handling using the `Pipeline` class by applying the `handle_dates` method.\n",
    "  - Returns the feature-engineered DataFrame.\n",
    "\n",
    "### Study Sources\n",
    "- For understanding feature engineering techniques: [Feature Engineering for Machine Learning](https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles/dp/1491953241)\n",
    "- Handling dates in Pandas: [Pandas DateTime Documentation](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)\n",
    "- Joining DataFrames in Pandas: [Pandas Merge Documentation](https://pandas.pydata.org/docs/reference/api/pandas.merge.html)\n",
    "- Applying functions to Pandas DataFrame using pipe: [Pandas Pipe Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9ec56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.528961Z",
     "iopub.status.busy": "2024-05-18T17:50:31.528689Z",
     "iopub.status.idle": "2024-05-18T17:50:31.534253Z",
     "shell.execute_reply": "2024-05-18T17:50:31.533469Z"
    },
    "papermill": {
     "duration": 0.022136,
     "end_time": "2024-05-18T17:50:31.536065",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.513929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b39b7",
   "metadata": {
    "papermill": {
     "duration": 0.013898,
     "end_time": "2024-05-18T17:50:31.564061",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.550163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ¼ DataFrame Conversion to Pandas with Categorical Columns \n",
    "\n",
    "The function `to_pandas(df_data, cat_cols=None)` converts a DataFrame (`df_data`) to a Pandas DataFrame and optionally converts specified columns to categorical data type. \n",
    "\n",
    " **1. `to_pandas(df_data, cat_cols=None)`**\n",
    "- **Inputs**:\n",
    "  - `df_data`: Input DataFrame to be converted to Pandas.\n",
    "  - `cat_cols`: Optional list of column names to be converted to categorical data type. Default is `None`.\n",
    "- **Output**: Returns the converted Pandas DataFrame and the list of categorical column names.\n",
    "- **Process**:\n",
    "  - Converts the input DataFrame to Pandas DataFrame using the `.to_pandas()` method.\n",
    "  - If `cat_cols` is not provided, it selects columns with data type \"object\" as default categorical columns.\n",
    "  - Converts the selected categorical columns to the categorical data type using `.astype(\"category\")`.\n",
    "  - Returns the converted Pandas DataFrame along with the list of categorical column names.\n",
    "\n",
    "### Study Sources\n",
    "- Converting Dask DataFrame to Pandas: [Dask DataFrame to Pandas](https://docs.dask.org/en/latest/dataframe-best-practices.html#converting-to-pandas)\n",
    "- Converting column data types in Pandas: [Pandas DataFrame astype](https://pandas.pydata.org/pandas-docs/version/1.3/reference/api/pandas.DataFrame.astype.html)\n",
    "- Handling categorical data in Pandas: [Categorical Data in Pandas](https://pandas.pydata.org/pandas-docs/version/1.3/user_guide/categorical.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15a281d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.593302Z",
     "iopub.status.busy": "2024-05-18T17:50:31.593058Z",
     "iopub.status.idle": "2024-05-18T17:50:31.597720Z",
     "shell.execute_reply": "2024-05-18T17:50:31.596866Z"
    },
    "papermill": {
     "duration": 0.02148,
     "end_time": "2024-05-18T17:50:31.599629",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.578149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e56d7d",
   "metadata": {
    "papermill": {
     "duration": 0.013877,
     "end_time": "2024-05-18T17:50:31.627873",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.613996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ”½ Memory Reduction Function for DataFrames \n",
    "\n",
    "The function `reduce_mem_usage(df)` iterates through all columns of a DataFrame and modifies the data types to reduce memory usage. \n",
    "\n",
    " **1. `reduce_mem_usage(df)`**\n",
    "- **Input**: \n",
    "  - `df`: Input DataFrame.\n",
    "- **Output**: Returns the DataFrame with reduced memory usage.\n",
    "- **Process**:\n",
    "  - Calculates the initial memory usage of the DataFrame (`start_mem`) using `df.memory_usage()`.\n",
    "  - Iterates through each column of the DataFrame:\n",
    "    - Checks if the column type is a category. If so, skips to the next column.\n",
    "    - For non-category columns:\n",
    "      - Determines the minimum and maximum values of the column (`c_min` and `c_max`).\n",
    "      - If the column type is integer:\n",
    "        - Checks if the data can be fit into `int8`, `int16`, `int32`, or `int64` and converts the column type accordingly.\n",
    "      - If the column type is float:\n",
    "        - Checks if the data can be fit into `float16`, `float32`, or `float64` and converts the column type accordingly.\n",
    "      - If the column type is object (string), it skips the conversion.\n",
    "  - Calculates the final memory usage of the DataFrame (`end_mem`) after the modifications.\n",
    "- **Returns** the DataFrame with reduced memory usage.\n",
    "\n",
    "### Study Sources\n",
    "- Optimizing memory usage in Pandas: [Optimizing Memory Usage in Pandas](https://www.dataquest.io/blog/pandas-big-data/)\n",
    "- Understanding data types and memory in Pandas: [Pandas Data Types and Memory Usage](https://pbpython.com/pandas_dtypes.html)\n",
    "- Data type conversion in NumPy: [NumPy Data Types](https://numpy.org/doc/stable/reference/arrays.scalars.html#arrays-scalars-built-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694b599c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.657672Z",
     "iopub.status.busy": "2024-05-18T17:50:31.657434Z",
     "iopub.status.idle": "2024-05-18T17:50:31.668998Z",
     "shell.execute_reply": "2024-05-18T17:50:31.668074Z"
    },
    "papermill": {
     "duration": 0.028723,
     "end_time": "2024-05-18T17:50:31.671054",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.642331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e071647",
   "metadata": {
    "papermill": {
     "duration": 0.014116,
     "end_time": "2024-05-18T17:50:31.699634",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.685518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " **Definition of Root Directory and Subdirectories**\n",
    "- `ROOT`: It specifies the root directory path where the dataset is located. The `Path` object is created using the `Path` class from the `pathlib` module.\n",
    "- `TRAIN_DIR`: It specifies the directory path for training data files. It is derived from the `ROOT` directory by appending the subdirectories \"parquet_files\" and \"train\" using the `/` operator.\n",
    "- `TEST_DIR`: It specifies the directory path for test data files. Similar to `TRAIN_DIR`, it is derived from the `ROOT` directory by appending the subdirectories \"parquet_files\" and \"test\" using the `/` operator.\n",
    "\n",
    "### Study Sources\n",
    "- Working with file paths in Python: [Pathlib Documentation](https://docs.python.org/3/library/pathlib.html)\n",
    "- Manipulating file paths using Pathlib: [Pathlib Tutorial](https://realpython.com/python-pathlib/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906a4b4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.729454Z",
     "iopub.status.busy": "2024-05-18T17:50:31.729177Z",
     "iopub.status.idle": "2024-05-18T17:50:31.733327Z",
     "shell.execute_reply": "2024-05-18T17:50:31.732604Z"
    },
    "papermill": {
     "duration": 0.02133,
     "end_time": "2024-05-18T17:50:31.735193",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.713863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb98ff",
   "metadata": {
    "papermill": {
     "duration": 0.014065,
     "end_time": "2024-05-18T17:50:31.763670",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.749605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination** \n",
    "\n",
    "\n",
    "Initializes a dictionary `data_store` containing different sets of DataFrames obtained from reading Parquet files using the `read_file()` and `read_files()` functions.\n",
    "\n",
    "****1. Data Store Initialization**\n",
    "- `data_store`: It is a dictionary storing different sets of DataFrames under different keys.\n",
    "  \n",
    "**2. Data Read Operations**\n",
    "- `df_base`: It stores the DataFrame obtained by reading the file \"train_base.parquet\" located in the `TRAIN_DIR` directory using the `read_file()` function.\n",
    "- `depth_0`: It stores a list of DataFrames obtained by reading multiple files. The first element is obtained using the `read_file()` function, while the second element is obtained using the `read_files()` function with a wildcard pattern.\n",
    "- `depth_1`: It stores a list of DataFrames obtained by reading multiple files using the `read_files()` function with specific patterns. Each file is associated with a depth level of 1.\n",
    "- `depth_2`: It stores a list containing a single DataFrame obtained by reading a specific file associated with a depth level of 2 using the `read_file()` function.\n",
    "\n",
    "### Study Sources\n",
    "- Loading data from Parquet files: [Dask Parquet Reader](https://docs.dask.org/en/latest/dataframe-io.html#dask.dataframe.read_parquet)\n",
    "- Working with dictionaries in Python: [Python Dictionaries](https://realpython.com/python-dicts/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7537a3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:50:31.794219Z",
     "iopub.status.busy": "2024-05-18T17:50:31.793428Z",
     "iopub.status.idle": "2024-05-18T17:51:41.536038Z",
     "shell.execute_reply": "2024-05-18T17:51:41.534961Z"
    },
    "papermill": {
     "duration": 69.760476,
     "end_time": "2024-05-18T17:51:41.538585",
     "exception": false,
     "start_time": "2024-05-18T17:50:31.778109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b9ae2",
   "metadata": {
    "papermill": {
     "duration": 0.014139,
     "end_time": "2024-05-18T17:51:41.567908",
     "exception": false,
     "start_time": "2024-05-18T17:51:41.553769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Data Preprocessing and Feature Engineering ðŸ”§\n",
    "\n",
    "Perform data preprocessing and feature engineering operations on the training data.\n",
    "\n",
    "**1. Data Preprocessing and Feature Engineering**\n",
    "- `df_train = feature_eng(**data_store)`: Applies feature engineering to the training data stored in the `data_store` dictionary using the `feature_eng` function. The unpacking operator `**` is used to pass the dictionary as keyword arguments.\n",
    "- `del data_store`: Deletes the `data_store` dictionary to release memory.\n",
    "- `gc.collect()`: Manually triggers garbage collection to free up memory space.\n",
    "\n",
    " **2. Data Filtering, Conversion, and Memory Reduction**\n",
    "- `df_train = df_train.pipe(Pipeline.filter_cols)`: Applies column filtering using the `filter_cols` method from the `Pipeline` class to the `df_train` DataFrame using the `pipe` method.\n",
    "- `df_train, cat_cols = to_pandas(df_train)`: Converts the `df_train` DataFrame to Pandas DataFrame and retrieves the categorical column names. It uses the `to_pandas` function for conversion.\n",
    "- `df_train = reduce_mem_usage(df_train)`: Reduces memory usage of the `df_train` DataFrame using the `reduce_mem_usage` function to optimize memory consumption.\n",
    "\n",
    " **3. Handling Missing Values**\n",
    "- `nums = df_train.select_dtypes(exclude='category').columns`: Selects numerical columns (excluding categorical columns) from the DataFrame and stores their column names in the `nums` variable.\n",
    "- `from itertools import combinations, permutations`: Imports the `combinations` and `permutations` functions from the `itertools` module.\n",
    "- `nans_df = df_train[nums].isna()`: Creates a DataFrame `nans_df` to identify missing values in numerical columns.\n",
    "- `nans_groups = {}`: Initializes an empty dictionary to store numerical columns grouped by the count of missing values.\n",
    "- Loops through each numerical column (`col`) and calculates the count of missing values for each column. Then, it groups the columns based on the count of missing values in the `nans_groups` dictionary.\n",
    "\n",
    " **4. Memory Management**\n",
    "- `del nans_df`: Deletes the `nans_df` DataFrame to release memory.\n",
    "- `x = gc.collect()`: Manually triggers garbage collection to free up memory space.\n",
    "\n",
    "### Study Sources\n",
    "- Data preprocessing techniques: [Data Preprocessing with Pandas](https://realpython.com/python-data-preprocessing/)\n",
    "- Feature engineering concepts: [Feature Engineering for Machine Learning](https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles/dp/1491953241)\n",
    "- Memory management in Python: [Memory Management in Python](https://realpython.com/python-memory-management/)\n",
    "- Missing data handling: [Handling Missing Data with Pandas](https://pandas.pydata.org/pandas-docs/version/1.3/user_guide/missing_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a043dc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:51:41.637905Z",
     "iopub.status.busy": "2024-05-18T17:51:41.637195Z",
     "iopub.status.idle": "2024-05-18T17:52:27.233461Z",
     "shell.execute_reply": "2024-05-18T17:52:27.232689Z"
    },
    "papermill": {
     "duration": 45.614338,
     "end_time": "2024-05-18T17:52:27.235849",
     "exception": false,
     "start_time": "2024-05-18T17:51:41.621511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = feature_eng(**data_store)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "nums=df_train.select_dtypes(exclude='category').columns\n",
    "from itertools import combinations, permutations\n",
    "nans_df = df_train[nums].isna()\n",
    "nans_groups={}\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "del nans_df; x=gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3d9ea",
   "metadata": {
    "papermill": {
     "duration": 0.015239,
     "end_time": "2024-05-18T17:52:27.266145",
     "exception": false,
     "start_time": "2024-05-18T17:52:27.250906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "Function `reduce_group(grps)` aims to reduce the number of columns within each group by selecting the column with the highest number of unique values. \n",
    "\n",
    "**1. `reduce_group(grps)`**\n",
    "- **Input**:\n",
    "  - `grps`: List of groups, where each group is represented as a list of column names.\n",
    "- **Output**: Returns a list of selected columns within each group.\n",
    "- **Process**:\n",
    "  - Initializes an empty list `use` to store the selected columns within each group.\n",
    "  - Iterates through each group `g` in the input list `grps`.\n",
    "    - Initializes variables `mx` and `vx` to track the maximum number of unique values and the corresponding column name within the group, respectively.\n",
    "    - Iterates through each column `gg` in the group `g`.\n",
    "      - Calculates the number of unique values `n` in the column `df_train[gg]`.\n",
    "      - Updates `mx` and `vx` if `n` is greater than the current maximum number of unique values.\n",
    "    - Appends the column name `vx` with the highest number of unique values to the `use` list for the current group.\n",
    "- **Returns** the list `use` containing selected columns within each group.\n",
    "\n",
    "### Study Sources\n",
    "- Working with groups and group operations: [Pandas GroupBy Documentation](https://pandas.pydata.org/pandas-docs/version/1.3/reference/groupby.html)\n",
    "- Unique values in Pandas Series: [Pandas nunique Documentation](https://pandas.pydata.org/pandas-docs/version/1.3/reference/api/pandas.Series.nunique.html)\n",
    "- Iterating through lists in Python: [Python List Iteration](https://realpython.com/iterate-through-dictionary-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b182c58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:52:27.296038Z",
     "iopub.status.busy": "2024-05-18T17:52:27.295727Z",
     "iopub.status.idle": "2024-05-18T17:52:27.301196Z",
     "shell.execute_reply": "2024-05-18T17:52:27.300318Z"
    },
    "papermill": {
     "duration": 0.022678,
     "end_time": "2024-05-18T17:52:27.303141",
     "exception": false,
     "start_time": "2024-05-18T17:52:27.280463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df_train[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "        use.append(vx)\n",
    "    return use\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431290d0",
   "metadata": {
    "papermill": {
     "duration": 0.014363,
     "end_time": "2024-05-18T17:52:27.332208",
     "exception": false,
     "start_time": "2024-05-18T17:52:27.317845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination** \n",
    "\n",
    "Function, `group_columns_by_correlation(matrix, threshold=0.8)`, aims to group columns based on their correlation values. \n",
    "\n",
    " **1. `group_columns_by_correlation(matrix, threshold=0.8)`**\n",
    "- **Inputs**:\n",
    "  - `matrix`: DataFrame representing the dataset.\n",
    "  - `threshold`: Threshold value for correlation. Columns with correlation values greater than or equal to this threshold will be grouped together. Default is set to 0.8.\n",
    "- **Output**: Returns a list of column groups where each group contains columns with correlation values above the specified threshold.\n",
    "- **Process**:\n",
    "  - Calculates the correlation matrix of the input DataFrame `matrix` using the `.corr()` method.\n",
    "  - Initializes an empty list `groups` to store the resulting column groups.\n",
    "  - Initializes a list `remaining_cols` containing all column names from the DataFrame.\n",
    "  - Iterates through each column `col` in the `remaining_cols` list:\n",
    "    - Initializes a group with the current column `col`.\n",
    "    - Initializes a list `correlated_cols` containing the current column `col`.\n",
    "    - Iterates through each remaining column `c` in the `remaining_cols` list:\n",
    "      - If the correlation between the current column `col` and column `c` is greater than or equal to the specified `threshold`, adds column `c` to the group and `correlated_cols`.\n",
    "    - Appends the current group to the `groups` list.\n",
    "    - Updates the `remaining_cols` list to exclude columns already correlated with the current column.\n",
    "- **Returns** the list of column groups.\n",
    "\n",
    "### Study Sources\n",
    "- Correlation matrix and its computation in Pandas: [Pandas Correlation Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)\n",
    "- Grouping data based on conditions: [Grouping Data in Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html)\n",
    "- Removing elements from a list in Python: [Python List Operations](https://www.programiz.com/python-programming/methods/list/remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62edccf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:52:27.362219Z",
     "iopub.status.busy": "2024-05-18T17:52:27.361943Z",
     "iopub.status.idle": "2024-05-18T17:52:27.368030Z",
     "shell.execute_reply": "2024-05-18T17:52:27.367261Z"
    },
    "papermill": {
     "duration": 0.023231,
     "end_time": "2024-05-18T17:52:27.369829",
     "exception": false,
     "start_time": "2024-05-18T17:52:27.346598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "    correlation_matrix = matrix.corr()\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    \n",
    "    return groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfb95b",
   "metadata": {
    "papermill": {
     "duration": 0.014381,
     "end_time": "2024-05-18T17:52:27.398644",
     "exception": false,
     "start_time": "2024-05-18T17:52:27.384263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ”„Handling Missing Values and Reducing Columns Based on Correlation\n",
    "\n",
    "Let's processes the `nans_groups` dictionary to handle missing values and reduce columns based on their correlation. \n",
    "### Explanation of Code\n",
    "\n",
    "1. **Initialization**\n",
    "   - `uses = []`: Initializes an empty list `uses` to store the final list of selected columns.\n",
    "\n",
    "2. **Iterate through Groups in `nans_groups`**\n",
    "   - `for k, v in nans_groups.items()`: Iterates through the `nans_groups` dictionary where `k` is the key (number of missing values) and `v` is the list of column names with that number of missing values.\n",
    "\n",
    "3. **Processing Each Group**\n",
    "   - **For Groups with More Than One Column**:\n",
    "     - `if len(v) > 1`: Checks if the group contains more than one column.\n",
    "       - `Vs = nans_groups[k]`: Assigns the list of columns `v` to `Vs`.\n",
    "       - `grps = group_columns_by_correlation(df_train[Vs], threshold=0.8)`: Groups columns in `Vs` based on their correlation using a threshold of 0.8.\n",
    "       - `use = reduce_group(grps)`: Reduces the groups by selecting columns with the highest number of unique values using the `reduce_group` function.\n",
    "       - `uses = uses + use`: Appends the selected columns to the `uses` list.\n",
    "   - **For Groups with a Single Column**:\n",
    "     - `else`: If the group contains only one column,\n",
    "       - `uses = uses + v`: Directly appends the column to the `uses` list.\n",
    "### Study Sources\n",
    "\n",
    "- **Handling Missing Data in Pandas**: [Pandas Documentation on Handling Missing Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n",
    "- **Correlation in Pandas**: [Pandas DataFrame.corr() Method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)\n",
    "- **Grouping Columns by Correlation**: [Correlation and Grouping in Data Analysis](https://machinelearningmastery.com/feature-selection-with-correlation-threshold/)\n",
    "- **Iterating and Modifying Lists in Python**: [Python List Operations](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "- **Memory Management in Python**: [Python Memory Management](https://realpython.com/python-memory-management/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934ffa68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:52:27.430350Z",
     "iopub.status.busy": "2024-05-18T17:52:27.430071Z",
     "iopub.status.idle": "2024-05-18T17:52:48.846419Z",
     "shell.execute_reply": "2024-05-18T17:52:48.845572Z"
    },
    "papermill": {
     "duration": 21.434645,
     "end_time": "2024-05-18T17:52:48.848756",
     "exception": false,
     "start_time": "2024-05-18T17:52:27.414111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "    else:\n",
    "        uses=uses+v\n",
    "\n",
    "# Subset the DataFrame to keep only the selected columns\n",
    "df_train = df_train[uses]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6975ca",
   "metadata": {
    "papermill": {
     "duration": 0.014544,
     "end_time": "2024-05-18T17:52:48.878381",
     "exception": false,
     "start_time": "2024-05-18T17:52:48.863837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ“ŠðŸš€ Data Preparation for Test Set ðŸš€ðŸ“Š\n",
    "\n",
    "Let's prepares the `data_store` dictionary for the test set by reading the required Parquet files. The structure and logic mirror those used for the training set, ensuring consistency in data preprocessing.\n",
    "### Explanation of Code\n",
    "\n",
    "1. **Reading Base and Depth Data for Test Set**:\n",
    "    - **Base Data**:\n",
    "      - `df_base`: Reads the base data from the Parquet file located at `TEST_DIR / \"test_base.parquet\"`.\n",
    "    - **Depth 0 Data**:\n",
    "      - `depth_0`: Reads the static credit bureau data from individual and wildcard Parquet files located at `TEST_DIR / \"test_static_cb_0.parquet\"` and `TEST_DIR / \"test_static_0_*.parquet\"`.\n",
    "    - **Depth 1 Data**:\n",
    "      - `depth_1`: Reads various related data files such as application previous, tax registry, credit bureau, and other related data files, all with depth 1, from their respective Parquet files.\n",
    "    - **Depth 2 Data**:\n",
    "      - `depth_2`: Reads the credit bureau data with depth 2 from the Parquet file located at `TEST_DIR / \"test_credit_bureau_b_2.parquet\"`.\n",
    "\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Reading Parquet Files**:\n",
    "   - [Pandas Documentation on Reading Parquet Files](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html)\n",
    "   - [Polars Documentation on Reading Parquet Files](https://pola-rs.github.io/polars/py-polars/html/reference/io.html#polars.read_parquet)\n",
    "2. **File Handling and Wildcards**:\n",
    "   - [Python glob Module](https://docs.python.org/3/library/glob.html)\n",
    "3. **Data Preprocessing Techniques**:\n",
    "   - [Pandas User Guide on Data Preprocessing](https://pandas.pydata.org/pandas-docs/stable/user_guide/preprocessing.html)\n",
    "   - [Polars User Guide on DataFrame Operations](https://pola-rs.github.io/polars/py-polars/html/user-guide/index.html)\n",
    "\n",
    "### Explanation of Concepts\n",
    "\n",
    "- **Parquet Files**: A columnar storage file format optimized for use with large-scale data processing frameworks.\n",
    "- **Data Preprocessing**: The process of transforming raw data into an understandable format. Includes reading data, handling missing values, and selecting relevant features.\n",
    "- **Wildcards in File Paths**: Used to specify patterns in file names. For example, `test_static_0_*.parquet` matches all files starting with `test_static_0_` and ending with `.parquet`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b5a9f5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:52:48.908603Z",
     "iopub.status.busy": "2024-05-18T17:52:48.908307Z",
     "iopub.status.idle": "2024-05-18T17:52:49.147348Z",
     "shell.execute_reply": "2024-05-18T17:52:49.146320Z"
    },
    "papermill": {
     "duration": 0.256692,
     "end_time": "2024-05-18T17:52:49.149532",
     "exception": false,
     "start_time": "2024-05-18T17:52:48.892840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd153c7",
   "metadata": {
    "papermill": {
     "duration": 0.014654,
     "end_time": "2024-05-18T17:52:49.179492",
     "exception": false,
     "start_time": "2024-05-18T17:52:49.164838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering and Data Preparation for the Test Set ðŸš€\n",
    "\n",
    "Performs feature engineering and data preparation on the test set. It follows the same steps as for the training set, ensuring consistency in data processing. \n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "    - `df_test = feature_eng(**data_store)`: Applies the `feature_eng` function to the test data stored in `data_store`. This function performs various feature engineering steps, such as creating new features and joining different depth data based on `case_id`.\n",
    "\n",
    "2. **Memory Management**:\n",
    "    - `del data_store`: Deletes the `data_store` dictionary to free up memory.\n",
    "    - `gc.collect()`: Calls the garbage collector to release any unreferenced memory.\n",
    "\n",
    "3. **Selecting Relevant Columns**:\n",
    "    - `df_test = df_test.select([col for col in df_train.columns if col != \"target\"])`: Selects columns in `df_test` that match the columns in `df_train`, excluding the \"target\" column. This ensures that the test set has the same features as the training set.\n",
    "\n",
    "4. **Conversion to Pandas DataFrame and Category Data Type**:\n",
    "    - `df_test, cat_cols = to_pandas(df_test)`: Converts the `df_test` Polars DataFrame to a Pandas DataFrame and converts specified columns to the \"category\" data type to save memory.\n",
    "\n",
    "5. **Memory Usage Reduction**:\n",
    "    - `df_test = reduce_mem_usage(df_test)`: Applies the `reduce_mem_usage` function to reduce the memory footprint of the Pandas DataFrame by converting columns to more efficient data types.\n",
    "\n",
    "6. **Final Memory Management**:\n",
    "    - `gc.collect()`: Calls the garbage collector again to release any unreferenced memory after data processing.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - [Feature Engineering for Machine Learning](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "2. **Memory Management**:\n",
    "   - [Python Memory Management](https://realpython.com/python-memory-management/)\n",
    "   - [Garbage Collection in Python](https://docs.python.org/3/library/gc.html)\n",
    "3. **DataFrame Selection**:\n",
    "   - [Pandas DataFrame Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\n",
    "4. **Data Type Conversion in Pandas**:\n",
    "   - [Pandas Data Types and Memory Usage](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes)\n",
    "\n",
    "### Explanation of Concepts\n",
    "\n",
    "- **Feature Engineering**: The process of using domain knowledge to create features (variables) that make machine learning algorithms work. It includes creating new features, transforming existing ones, and joining data from different sources.\n",
    "- **Garbage Collection**: A form of automatic memory management that reclaims memory occupied by objects no longer in use by the program.\n",
    "- **Data Type Conversion**: Changing the data type of columns to more efficient types (e.g., from `float64` to `float32`) to save memory and improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0063dea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:52:49.209725Z",
     "iopub.status.busy": "2024-05-18T17:52:49.209436Z",
     "iopub.status.idle": "2024-05-18T17:52:49.623777Z",
     "shell.execute_reply": "2024-05-18T17:52:49.622758Z"
    },
    "papermill": {
     "duration": 0.432012,
     "end_time": "2024-05-18T17:52:49.626050",
     "exception": false,
     "start_time": "2024-05-18T17:52:49.194038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = feature_eng(**data_store)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n",
    "df_test, cat_cols = to_pandas(df_test)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44ada7",
   "metadata": {
    "papermill": {
     "duration": 0.015712,
     "end_time": "2024-05-18T17:52:49.658726",
     "exception": false,
     "start_time": "2024-05-18T17:52:49.643014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **Adding Target Column to Training Set**:\n",
    "    - `df_train['target'] = 0`: Adds a column named \"target\" to the `df_train` DataFrame and sets its value to 0 for all rows. This indicates that these rows belong to the training set.\n",
    "\n",
    "2. **Adding Target Column to Test Set**:\n",
    "    - `df_test['target'] = 1`: Adds a column named \"target\" to the `df_test` DataFrame and sets its value to 1 for all rows. This indicates that these rows belong to the test set.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Adding Columns in Pandas**:\n",
    "   - [Pandas DataFrame Adding Columns](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#adding-columns)\n",
    "2. **Combining Train and Test Data for Preprocessing**:\n",
    "   - [Combining Datasets for Preprocessing](https://machinelearningmastery.com/combining-train-and-test-datasets-for-preparing-machine-learning-data/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a7603f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:52:49.692391Z",
     "iopub.status.busy": "2024-05-18T17:52:49.692071Z",
     "iopub.status.idle": "2024-05-18T17:52:49.698807Z",
     "shell.execute_reply": "2024-05-18T17:52:49.697893Z"
    },
    "papermill": {
     "duration": 0.026093,
     "end_time": "2024-05-18T17:52:49.700813",
     "exception": false,
     "start_time": "2024-05-18T17:52:49.674720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['target']=0\n",
    "df_test['target']=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10177d",
   "metadata": {
    "papermill": {
     "duration": 0.015276,
     "end_time": "2024-05-18T17:52:49.731979",
     "exception": false,
     "start_time": "2024-05-18T17:52:49.716703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Combining and Preparing Data for Modeling ðŸš€ðŸ“Š\n",
    "\n",
    "Let's combines the training and test datasets, optimizes memory usage, prepares the features and target for modeling, and then saves the prepared data to a file using `joblib`. \n",
    "### Explanation of Code\n",
    "\n",
    "1. **Combining Train and Test Data**:\n",
    "    - `df_train = pd.concat([df_train, df_test])`: Concatenates the training and test datasets along the rows. This step combines the datasets into one for uniform preprocessing.\n",
    "\n",
    "2. **Reducing Memory Usage**:\n",
    "    - `df_train = reduce_mem_usage(df_train)`: Applies the `reduce_mem_usage` function to the combined DataFrame to optimize its memory usage by converting columns to more efficient data types.\n",
    "\n",
    "3. **Preparing Target Variable**:\n",
    "    - `y = df_train[\"target\"]`: Extracts the \"target\" column from the combined DataFrame and stores it in the variable `y`. This will be used as the target variable for modeling.\n",
    "\n",
    "4. **Dropping Unnecessary Columns**:\n",
    "    - `df_train = df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])`: Drops the \"target\", \"case_id\", and \"WEEK_NUM\" columns from the combined DataFrame. The \"case_id\" and \"WEEK_NUM\" columns are likely identifiers and not useful for modeling.\n",
    "\n",
    "5. **Saving the Prepared Data**:\n",
    "    - `joblib.dump((df_train, y, df_test), 'data.pkl')`: Uses `joblib` to save the prepared features (`df_train`), target (`y`), and test set (`df_test`) to a file named `data.pkl`. This serialized file can be loaded later for model training and evaluation.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Pandas Concatenation**:\n",
    "   - [Pandas Concatenation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)\n",
    "2. **Memory Optimization in Pandas**:\n",
    "   - [Pandas Memory Usage](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets)\n",
    "3. **Joblib for Serialization**:\n",
    "   - [Joblib Documentation](https://joblib.readthedocs.io/en/latest/)\n",
    "4. **Data Preparation for Machine Learning**:\n",
    "   - [Data Preparation Techniques](https://machinelearningmastery.com/data-preparation-techniques-for-machine-learning/)\n",
    "\n",
    "### Explanation of Concepts\n",
    "\n",
    "- **Data Concatenation**: Combining multiple DataFrames along a particular axis (rows or columns).\n",
    "- **Memory Optimization**: Techniques to reduce the memory footprint of data structures, crucial for handling large datasets efficiently.\n",
    "- **Target Variable**: The variable that a model aims to predict. Here, it distinguishes between training (0) and test (1) data.\n",
    "- **Serialization**: The process of converting a data structure into a format that can be easily saved to disk and later restored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16bb9872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:52:49.764714Z",
     "iopub.status.busy": "2024-05-18T17:52:49.764454Z",
     "iopub.status.idle": "2024-05-18T17:53:01.234022Z",
     "shell.execute_reply": "2024-05-18T17:53:01.233142Z"
    },
    "papermill": {
     "duration": 11.488222,
     "end_time": "2024-05-18T17:53:01.236102",
     "exception": false,
     "start_time": "2024-05-18T17:52:49.747880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.concat([df_train,df_test])\n",
    "df_train=reduce_mem_usage(df_train)\n",
    "\n",
    "y = df_train[\"target\"]\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "\n",
    "\n",
    "joblib.dump((df_train,y,df_test),'data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f9c7d",
   "metadata": {
    "papermill": {
     "duration": 0.014778,
     "end_time": "2024-05-18T17:53:01.266430",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.251652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a341cf",
   "metadata": {
    "papermill": {
     "duration": 0.014782,
     "end_time": "2024-05-18T17:53:01.296150",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.281368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Data Preprocessing with Pipeline Class ðŸš€\n",
    "\n",
    "Define a `Pipeline` class with methods to preprocess a DataFrame. The class includes methods for setting data types, handling date columns, and filtering columns based on certain criteria.\n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **set_table_dtypes(df)**:\n",
    "    - This method sets the appropriate data types for the columns in the DataFrame.\n",
    "    - **Int64**: Converts specified columns to 64-bit integers.\n",
    "    - **Date**: Converts specified columns to date type.\n",
    "    - **Float64**: Converts specified columns to 64-bit floating-point numbers.\n",
    "    - **String**: Converts specified columns to string type.\n",
    "  \n",
    "2. **handle_dates(df)**:\n",
    "    - This method handles date columns by calculating the difference in days between date columns ending with \"D\" and a reference date column \"date_decision\".\n",
    "    - It drops the \"date_decision\" and \"MONTH\" columns after the calculations.\n",
    "\n",
    "3. **filter_cols(df)**:\n",
    "    - This method filters out columns based on missing values and unique values.\n",
    "    - Columns with more than 70% missing values are dropped.\n",
    "    - String columns with either only one unique value or more than 200 unique values are dropped as they are likely not useful for modeling.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Polars Documentation**:\n",
    "   - [Polars User Guide](https://pola-rs.github.io/polars/py-polars/html/index.html)\n",
    "2. **Handling Missing Data**:\n",
    "   - [Dealing with Missing Data in Machine Learning](https://machinelearningmastery.com/handle-missing-data-python/)\n",
    "3. **Data Types in Python**:\n",
    "   - [Python Data Types](https://docs.python.org/3/library/datatypes.html)\n",
    "4. **Date and Time Handling**:\n",
    "   - [Working with Dates and Times in Python](https://realpython.com/python-datetime/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40c2bc0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:01.328329Z",
     "iopub.status.busy": "2024-05-18T17:53:01.328055Z",
     "iopub.status.idle": "2024-05-18T17:53:01.339880Z",
     "shell.execute_reply": "2024-05-18T17:53:01.339024Z"
    },
    "papermill": {
     "duration": 0.030589,
     "end_time": "2024-05-18T17:53:01.341753",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.311164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.7:\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c9388",
   "metadata": {
    "papermill": {
     "duration": 0.015042,
     "end_time": "2024-05-18T17:53:01.371827",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.356785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Aggregation with Aggregator Class ðŸ”§\n",
    "\n",
    "Define an `Aggregator` class designed to aggregate features from a DataFrame. Aggregation functions are used to transform and summarize data, which can help in creating new features for machine learning models. \n",
    "### Explanation of Code\n",
    "\n",
    "1. **num_expr(df)**:\n",
    "    - This method aggregates numerical columns ending with \"P\" or \"A\".\n",
    "    - **Max**: Maximum value of each column.\n",
    "    - **Last**: Last value of each column.\n",
    "    - **Mean**: Average value of each column.\n",
    "    - **Median** and **Variance** are also defined but not used in this implementation.\n",
    "\n",
    "2. **date_expr(df)**:\n",
    "    - This method aggregates date columns ending with \"D\".\n",
    "    - **Max**: Latest date in each column.\n",
    "    - **Last**: Last date in each column.\n",
    "    - **Mean**: Average date in each column.\n",
    "\n",
    "3. **str_expr(df)**:\n",
    "    - This method aggregates string columns ending with \"M\".\n",
    "    - **Max**: Lexicographically last string in each column.\n",
    "    - **Last**: Last string value in each column.\n",
    "\n",
    "4. **other_expr(df)**:\n",
    "    - This method aggregates other columns ending with \"T\" or \"L\".\n",
    "    - **Max**: Maximum value in each column.\n",
    "    - **Last**: Last value in each column.\n",
    "\n",
    "5. **count_expr(df)**:\n",
    "    - This method aggregates columns containing \"num_group\".\n",
    "    - **Max**: Maximum value in each column.\n",
    "    - **Last**: Last value in each column.\n",
    "\n",
    "6. **get_exprs(df)**:\n",
    "    - This method combines all the aggregation expressions from the above methods.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Polars Documentation**:\n",
    "   - [Polars User Guide](https://pola-rs.github.io/polars/py-polars/html/index.html)\n",
    "2. **Feature Engineering**:\n",
    "   - [Feature Engineering for Machine Learning](https://www.udacity.com/course/feature-engineering-for-machine-learning--nd025)\n",
    "3. **Data Aggregation and Group Operations**:\n",
    "   - [Pandas GroupBy: Your Guide to Grouping Data in Python](https://realpython.com/pandas-groupby/)\n",
    "4. **Correlation in Python**:\n",
    "   - [How to Calculate Correlation in Python](https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/)\n",
    "\n",
    "### Explanation of Concepts\n",
    "\n",
    "- **Aggregation Functions**: These functions summarize data by calculating statistical measures like max, mean, and last value, which can provide meaningful insights for machine learning models.\n",
    "- **Feature Engineering**: The process of using domain knowledge to create features that make machine learning algorithms work better.\n",
    "- **Data Types and Polars**: Understanding and using correct data types and efficient data manipulation libraries like Polars is crucial for handling large datasets effectively.\n",
    "- **Correlation**: Measures the relationship between two variables, helping in feature selection by identifying highly correlated features that may provide redundant information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec924b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:01.403750Z",
     "iopub.status.busy": "2024-05-18T17:53:01.402929Z",
     "iopub.status.idle": "2024-05-18T17:53:01.418046Z",
     "shell.execute_reply": "2024-05-18T17:53:01.417148Z"
    },
    "papermill": {
     "duration": 0.033036,
     "end_time": "2024-05-18T17:53:01.419902",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.386866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n",
    "        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        # expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "        return expr_max + expr_last  # +expr_count\n",
    "\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d5fdd",
   "metadata": {
    "papermill": {
     "duration": 0.01515,
     "end_time": "2024-05-18T17:53:01.450237",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.435087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation Functions \n",
    "\n",
    "Functions to prepare and process data for our  machine learning pipeline. These functions include reading data from files, performing feature engineering, and converting data formats.\n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **read_file(path, depth=None)**:\n",
    "    - **Purpose**: Reads a Parquet file, sets appropriate data types, and optionally performs aggregation based on `depth`.\n",
    "    - **Parameters**:\n",
    "        - `path`: File path to the Parquet file.\n",
    "        - `depth`: Determines the level of aggregation (1 or 2).\n",
    "    - **Process**:\n",
    "        - Reads the Parquet file into a DataFrame.\n",
    "        - Sets the appropriate data types using a pipeline.\n",
    "        - If `depth` is 1 or 2, groups the data by `case_id` and aggregates features using the `Aggregator` class.\n",
    "\n",
    "2. **read_files(regex_path, depth=None)**:\n",
    "    - **Purpose**: Reads multiple Parquet files matching a regex pattern, sets appropriate data types, and optionally performs aggregation.\n",
    "    - **Parameters**:\n",
    "        - `regex_path`: Regex pattern to match file paths.\n",
    "        - `depth`: Determines the level of aggregation (1 or 2).\n",
    "    - **Process**:\n",
    "        - Reads each Parquet file matching the regex pattern into a DataFrame.\n",
    "        - Sets the appropriate data types using a pipeline.\n",
    "        - If `depth` is 1 or 2, groups the data by `case_id` and aggregates features.\n",
    "        - Concatenates the DataFrames vertically and removes duplicate `case_id`s.\n",
    "\n",
    "3. **feature_eng(df_base, depth_0, depth_1, depth_2)**:\n",
    "    - **Purpose**: Performs feature engineering by adding date-related features and joining additional data based on `case_id`.\n",
    "    - **Parameters**:\n",
    "        - `df_base`: Base DataFrame.\n",
    "        - `depth_0`, `depth_1`, `depth_2`: Lists of DataFrames at different depths.\n",
    "    - **Process**:\n",
    "        - Adds `month_decision` and `weekday_decision` features based on `date_decision`.\n",
    "        - Joins additional DataFrames from `depth_0`, `depth_1`, and `depth_2` to the base DataFrame.\n",
    "        - Handles date columns using a pipeline.\n",
    "\n",
    "4. **to_pandas(df_data, cat_cols=None)**:\n",
    "    - **Purpose**: Converts a Polars DataFrame to a Pandas DataFrame and sets categorical data types.\n",
    "    - **Parameters**:\n",
    "        - `df_data`: Polars DataFrame to be converted.\n",
    "        - `cat_cols`: List of columns to be converted to categorical type.\n",
    "    - **Process**:\n",
    "        - Converts the Polars DataFrame to a Pandas DataFrame.\n",
    "        - If `cat_cols` is not provided, identifies columns of object type and converts them to categorical.\n",
    "        - Returns the converted DataFrame and the list of categorical columns.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Polars Documentation**:\n",
    "   - [Polars User Guide](https://pola-rs.github.io/polars/py-polars/html/index.html)\n",
    "2. **Pandas Documentation**:\n",
    "   - [Pandas User Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)\n",
    "3. **Feature Engineering for Machine Learning**:\n",
    "   - [Feature Engineering for Machine Learning](https://www.udacity.com/course/feature-engineering-for-machine-learning--nd025)\n",
    "4. **Data Aggregation and Group Operations in Pandas**:\n",
    "   - [Pandas GroupBy: Your Guide to Grouping Data in Python](https://realpython.com/pandas-groupby/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0f4af87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:01.482322Z",
     "iopub.status.busy": "2024-05-18T17:53:01.482060Z",
     "iopub.status.idle": "2024-05-18T17:53:01.492532Z",
     "shell.execute_reply": "2024-05-18T17:53:01.491710Z"
    },
    "papermill": {
     "duration": 0.028835,
     "end_time": "2024-05-18T17:53:01.494356",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.465521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b692e5",
   "metadata": {
    "papermill": {
     "duration": 0.016009,
     "end_time": "2024-05-18T17:53:01.525761",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.509752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ› ï¸ Memory Optimization Function\n",
    "\n",
    "Function `reduce_mem_usage(df)` which takes a DataFrame `df` as input and iterates through all its columns. The purpose of this function is to optimize the memory usage of the DataFrame by adjusting the data types of its columns.\n",
    "**Explanation:**\n",
    "\n",
    "\n",
    "- The function starts by calculating the initial memory usage of the DataFrame `df`.\n",
    "- It then iterates through each column of the DataFrame.\n",
    "- For each column, it checks the data type.\n",
    "- If the column is categorical, it skips the optimization process.\n",
    "- For non-categorical columns, it finds the minimum and maximum values.\n",
    "- Based on the range of values, it changes the data type to one that requires less memory while ensuring that it can still accommodate the data without loss of precision.\n",
    "- Finally, it calculates the memory usage after optimization and prints out the reduction percentage.\n",
    "- The function returns the optimized DataFrame.\n",
    "\n",
    "**Study Sources:**\n",
    "1. Pandas Documentation: https://pandas.pydata.org/docs/\n",
    "2. NumPy Documentation: https://numpy.org/doc/stable/\n",
    "3. Data Type Objects (dtype) - NumPy Documentation: https://numpy.org/doc/stable/reference/arrays.dtypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eb581a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:01.557314Z",
     "iopub.status.busy": "2024-05-18T17:53:01.557042Z",
     "iopub.status.idle": "2024-05-18T17:53:01.568834Z",
     "shell.execute_reply": "2024-05-18T17:53:01.568042Z"
    },
    "papermill": {
     "duration": 0.029731,
     "end_time": "2024-05-18T17:53:01.570736",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.541005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93539b0d",
   "metadata": {
    "papermill": {
     "duration": 0.01508,
     "end_time": "2024-05-18T17:53:01.601219",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.586139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Information Retrieval \n",
    "\n",
    "**Explanation:**\n",
    "- The code loads information about a specific LightGBM model from a file named 'notebook_info.joblib'.\n",
    "- It prints out the start time of the notebook that created the models and a brief description of the notebook.\n",
    "- The code then retrieves details about the columns and categorical columns used in the models.\n",
    "- It prints out the number of columns and categorical columns.\n",
    "- Next, it loads the LightGBM models from a file named 'lgb_models.joblib'.\n",
    "- Finally, it displays the loaded LightGBM models.\n",
    "\n",
    "- Similarly, this part of the code loads information about categorical (cat) models and prints out the start time of the notebook that created the models and a brief description of the notebook.\n",
    "- It then loads the categorical (cat) models from a file named 'cat_models.joblib'.\n",
    "- Finally, it displays the loaded categorical models.\n",
    "\n",
    "**Study Sources:**\n",
    "1. Joblib Documentation: https://joblib.readthedocs.io/en/latest/\n",
    "2. LightGBM Documentation: https://lightgbm.readthedocs.io/en/latest/\n",
    "3. Categorical Features in Machine Learning: https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72ddccd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:01.632833Z",
     "iopub.status.busy": "2024-05-18T17:53:01.632569Z",
     "iopub.status.idle": "2024-05-18T17:53:06.663971Z",
     "shell.execute_reply": "2024-05-18T17:53:06.662991Z"
    },
    "papermill": {
     "duration": 5.049441,
     "end_time": "2024-05-18T17:53:06.666002",
     "exception": false,
     "start_time": "2024-05-18T17:53:01.616561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [lgb] notebook_start_time: 2024-04-17 17:19:35.710340\n",
      "- [lgb] description: Add notebook info dict to store cols and cat_cols\n",
      "- [lgb] len(cols): 386\n",
      "- [lgb] len(cat_cols): 113\n",
      "- [cat] notebook_start_time: 2024-04-18 00:37:32.864485\n",
      "- [cat] description: first cat models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<catboost.core.CatBoostClassifier at 0x7873426f3f70>,\n",
       " <catboost.core.CatBoostClassifier at 0x787360c30100>,\n",
       " <catboost.core.CatBoostClassifier at 0x78735a6a5d20>,\n",
       " <catboost.core.CatBoostClassifier at 0x78735a6c26e0>,\n",
       " <catboost.core.CatBoostClassifier at 0x78733eb01210>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/notebook_info.joblib')\n",
    "print(f\"- [lgb] notebook_start_time: {lgb_notebook_info['notebook_start_time']}\")\n",
    "print(f\"- [lgb] description: {lgb_notebook_info['description']}\")\n",
    "\n",
    "cols = lgb_notebook_info['cols']\n",
    "cat_cols = lgb_notebook_info['cat_cols']\n",
    "print(f\"- [lgb] len(cols): {len(cols)}\")\n",
    "print(f\"- [lgb] len(cat_cols): {len(cat_cols)}\")\n",
    "\n",
    "lgb_models = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/lgb_models.joblib')\n",
    "lgb_models\n",
    "\n",
    "cat_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/notebook_info.joblib')\n",
    "print(f\"- [cat] notebook_start_time: {cat_notebook_info['notebook_start_time']}\")\n",
    "print(f\"- [cat] description: {cat_notebook_info['description']}\")\n",
    "\n",
    "cat_models = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/cat_models.joblib')\n",
    "cat_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543e69e",
   "metadata": {
    "papermill": {
     "duration": 0.015889,
     "end_time": "2024-05-18T17:53:06.698112",
     "exception": false,
     "start_time": "2024-05-18T17:53:06.682223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ“‚ðŸ” Data Loading and Storage Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf39226",
   "metadata": {
    "papermill": {
     "duration": 0.015354,
     "end_time": "2024-05-18T17:53:06.728977",
     "exception": false,
     "start_time": "2024-05-18T17:53:06.713623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Sets up a directory structure for storing and accessing test data related to credit risk models.\n",
    "- It defines `ROOT` as the root directory and `TEST_DIR` as the directory containing test data in parquet format.\n",
    "- The `data_store` dictionary is initialized to store different types of test data.\n",
    "- Each key in the `data_store` dictionary corresponds to a different depth level of data.\n",
    "- For each depth level, specific files are read into memory using the `read_file` and `read_files` functions, and these data are stored as lists under the corresponding depth level keys in the `data_store` dictionary.\n",
    "\n",
    "**Study Sources:**\n",
    "1. Pathlib Documentation: https://docs.python.org/3/library/pathlib.html\n",
    "2. Parquet File Format: https://parquet.apache.org/documentation/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc11717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:06.761487Z",
     "iopub.status.busy": "2024-05-18T17:53:06.761165Z",
     "iopub.status.idle": "2024-05-18T17:53:06.971506Z",
     "shell.execute_reply": "2024-05-18T17:53:06.970723Z"
    },
    "papermill": {
     "duration": 0.229009,
     "end_time": "2024-05-18T17:53:06.973598",
     "exception": false,
     "start_time": "2024-05-18T17:53:06.744589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66502761",
   "metadata": {
    "papermill": {
     "duration": 0.015583,
     "end_time": "2024-05-18T17:53:07.005193",
     "exception": false,
     "start_time": "2024-05-18T17:53:06.989610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "- Performs feature engineering on the test data using the `feature_eng` function with the provided data store.\n",
    "- After feature engineering, it prints out the shape of the processed test data.\n",
    "- Then, it deletes the data store and performs garbage collection to free up memory.\n",
    "- It selects only the required columns from the processed test data.\n",
    "- The test data is converted to a pandas DataFrame and categorical columns are handled accordingly.\n",
    "- Memory usage of the test data is optimized using the `reduce_mem_usage` function.\n",
    "- Finally, the 'case_id' column is set as the index for the test data, and its shape is printed out again.\n",
    "\n",
    "**Study Sources:**\n",
    "1. Garbage Collection in Python: https://docs.python.org/3/library/gc.html\n",
    "2. Pandas DataFrame Selection: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html\n",
    "3. Memory Optimization Techniques in Pandas: https://www.dataquest.io/blog/pandas-big-data/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4de5e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:07.037580Z",
     "iopub.status.busy": "2024-05-18T17:53:07.037315Z",
     "iopub.status.idle": "2024-05-18T17:53:07.515332Z",
     "shell.execute_reply": "2024-05-18T17:53:07.514424Z"
    },
    "papermill": {
     "duration": 0.496617,
     "end_time": "2024-05-18T17:53:07.517395",
     "exception": false,
     "start_time": "2024-05-18T17:53:07.020778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data shape:\t (10, 860)\n",
      "Memory usage of dataframe is 0.04 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 40.2%\n",
      "test data shape:\t (10, 386)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = feature_eng(**data_store)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "df_test = df_test.select(['case_id'] + cols)\n",
    "\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "df_test = df_test.set_index('case_id')\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad2bdf",
   "metadata": {
    "papermill": {
     "duration": 0.017028,
     "end_time": "2024-05-18T17:53:07.550906",
     "exception": false,
     "start_time": "2024-05-18T17:53:07.533878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "- Define a custom ensemble model named `VotingModel` that inherits from `BaseEstimator` and `RegressorMixin`.\n",
    "- The `__init__` method initializes the model with a list of estimators (models) to be used for voting aggregation.\n",
    "- The `fit` method is implemented but does nothing, as fitting is not required for voting aggregation.\n",
    "- The `predict` method performs prediction using voting aggregation on the provided features by averaging predictions from all estimators.\n",
    "- The `predict_proba` method performs prediction with probabilities using voting aggregation on the provided features.\n",
    "- For prediction with probabilities, it first collects predictions from LightGBM (lgb) models and then from categorical (cat) models. Categorical columns are converted to string type before making predictions to ensure compatibility.\n",
    "- Finally, the predictions are averaged across all models to get the final predicted probabilities.\n",
    "\n",
    "**Study Sources:**\n",
    "1. Scikit-learn BaseEstimator Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html\n",
    "2. Scikit-learn RegressorMixin Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "468fe8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:07.584387Z",
     "iopub.status.busy": "2024-05-18T17:53:07.584104Z",
     "iopub.status.idle": "2024-05-18T17:53:07.593977Z",
     "shell.execute_reply": "2024-05-18T17:53:07.593241Z"
    },
    "papermill": {
     "duration": 0.029029,
     "end_time": "2024-05-18T17:53:07.595880",
     "exception": false,
     "start_time": "2024-05-18T17:53:07.566851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A custom ensemble model that performs voting aggregation for predictions.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    estimators : list\n",
    "        List of estimators (models) to be used for voting aggregation.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(X, y=None):\n",
    "        Fit the ensemble model. This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "    predict(X):\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "    predict_proba(X):\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators):\n",
    "        \"\"\"\n",
    "        Initialize the VotingModel.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        estimators : list\n",
    "            List of estimators (models) to be used for voting aggregation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the ensemble model.\n",
    "\n",
    "        This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,) (default=None)\n",
    "            Target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array-like, shape (n_samples,)\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "     \n",
    "    def predict_proba(self, X):      \n",
    "        \"\"\"\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred_proba : array-like, shape (n_samples, n_classes)\n",
    "            Predicted probabilities.\n",
    "        \"\"\"\n",
    "        # lgb\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n",
    "        \n",
    "        # cat\n",
    "        X[cat_cols] = X[cat_cols].astype(str)\n",
    "        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[-5:]]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21ad304d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:07.628921Z",
     "iopub.status.busy": "2024-05-18T17:53:07.628634Z",
     "iopub.status.idle": "2024-05-18T17:53:07.634145Z",
     "shell.execute_reply": "2024-05-18T17:53:07.633342Z"
    },
    "papermill": {
     "duration": 0.024064,
     "end_time": "2024-05-18T17:53:07.635928",
     "exception": false,
     "start_time": "2024-05-18T17:53:07.611864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VotingModel(lgb_models + cat_models)\n",
    "len(model.estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3143db47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:07.669673Z",
     "iopub.status.busy": "2024-05-18T17:53:07.669436Z",
     "iopub.status.idle": "2024-05-18T17:53:08.714329Z",
     "shell.execute_reply": "2024-05-18T17:53:08.713525Z"
    },
    "papermill": {
     "duration": 1.064442,
     "end_time": "2024-05-18T17:53:08.716522",
     "exception": false,
     "start_time": "2024-05-18T17:53:07.652080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n",
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "df_subm[\"score\"] = y_pred\n",
    "df_subm.to_csv(\"sub.csv\")\n",
    "df_train,y,df_test=joblib.load('/kaggle/working/data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027e192",
   "metadata": {
    "papermill": {
     "duration": 0.016065,
     "end_time": "2024-05-18T17:53:08.749427",
     "exception": false,
     "start_time": "2024-05-18T17:53:08.733362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "791e6122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:53:08.783413Z",
     "iopub.status.busy": "2024-05-18T17:53:08.783122Z",
     "iopub.status.idle": "2024-05-18T17:54:48.219627Z",
     "shell.execute_reply": "2024-05-18T17:54:48.218678Z"
    },
    "papermill": {
     "duration": 99.456391,
     "end_time": "2024-05-18T17:54:48.222037",
     "exception": false,
     "start_time": "2024-05-18T17:53:08.765646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10, number of negative: 1526659\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.088825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46717\n",
      "[LightGBM] [Info] Number of data points in the train set: 1526669, number of used features: 308\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000007 -> initscore=-11.936007\n",
      "[LightGBM] [Info] Start training from score -11.936007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "fitted_models_lgb=[]\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(df_train,y)\n",
    "fitted_models_lgb.append(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce70e10",
   "metadata": {
    "papermill": {
     "duration": 0.018392,
     "end_time": "2024-05-18T17:54:48.260783",
     "exception": false,
     "start_time": "2024-05-18T17:54:48.242391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "- Defines a custom ensemble model named `VotingModel` for voting aggregation of predictions.\n",
    "- It inherits from `BaseEstimator` and `RegressorMixin`.\n",
    "- The `__init__` method initializes the model with a list of fitted estimators (models) to be used for voting aggregation.\n",
    "- The `fit` method is implemented but does nothing, as fitting is not required for voting aggregation.\n",
    "- The `predict` method performs prediction using voting aggregation on the provided features by averaging predictions from all fitted estimators.\n",
    "- The `predict_proba` method performs prediction with probabilities using voting aggregation on the provided features by averaging probabilities from all fitted estimators.\n",
    "- An instance of `VotingModel` is then created with a list of fitted LightGBM models (`fitted_models_lgb`).\n",
    "\n",
    "This custom ensemble model allows for easy integration of multiple fitted models for voting aggregation of predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "254cb9e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:54:48.299544Z",
     "iopub.status.busy": "2024-05-18T17:54:48.298627Z",
     "iopub.status.idle": "2024-05-18T17:54:48.308050Z",
     "shell.execute_reply": "2024-05-18T17:54:48.307214Z"
    },
    "papermill": {
     "duration": 0.030769,
     "end_time": "2024-05-18T17:54:48.309968",
     "exception": false,
     "start_time": "2024-05-18T17:54:48.279199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A custom ensemble model for voting aggregation of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    estimators : list\n",
    "        List of fitted estimators (models) to be used for voting aggregation.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(X, y=None):\n",
    "        Fit the ensemble model. This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "    predict(X):\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "    predict_proba(X):\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators):\n",
    "        \"\"\"\n",
    "        Initialize the VotingModel.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        estimators : list\n",
    "            List of fitted estimators (models) to be used for voting aggregation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the ensemble model.\n",
    "\n",
    "        This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,) (default=None)\n",
    "            Target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array-like, shape (n_samples,)\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred_proba : array-like, shape (n_samples, n_classes)\n",
    "            Predicted probabilities.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "model = VotingModel(fitted_models_lgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d145e",
   "metadata": {
    "papermill": {
     "duration": 0.01841,
     "end_time": "2024-05-18T17:54:48.346752",
     "exception": false,
     "start_time": "2024-05-18T17:54:48.328342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "- Drops columns \"WEEK_NUM\" and 'target' from the test data DataFrame `df_test` as they are not required for prediction.\n",
    "- The 'case_id' column is set as the index for `df_test` DataFrame.\n",
    "- Predictions are made on the test data using the ensemble model (`model`), and probabilities for class 1 are extracted.\n",
    "- A condition is defined based on the predicted probabilities.\n",
    "- The submission file is read into `df_subm` DataFrame, and the 'case_id' column is set as its index.\n",
    "- Scores in the submission DataFrame are adjusted based on the defined condition.\n",
    "- The modified submission DataFrame is saved to a CSV file named \"submission.csv\".\n",
    "- Finally, a file named 'data.pkl' is removed from the working directory.\n",
    "\n",
    "This code snippet completes the process of generating predictions, adjusting scores based on a condition, and saving the modified submission to a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91cdf379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T17:54:48.385284Z",
     "iopub.status.busy": "2024-05-18T17:54:48.385009Z",
     "iopub.status.idle": "2024-05-18T17:54:49.671322Z",
     "shell.execute_reply": "2024-05-18T17:54:49.670066Z"
    },
    "papermill": {
     "duration": 1.308135,
     "end_time": "2024-05-18T17:54:49.673723",
     "exception": false,
     "start_time": "2024-05-18T17:54:48.365588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.drop(columns=[\"WEEK_NUM\",'target'])\n",
    "df_test = df_test.set_index(\"case_id\")\n",
    "\n",
    "y_pred = pd.Series(model.predict_proba(df_test)[:,1], index=df_test.index)\n",
    "condition=y_pred<0.98\n",
    "df_subm = pd.read_csv(\"/kaggle/working/sub.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm.loc[condition, 'score'] = (df_subm.loc[condition, 'score'] - 0.073).clip(0)\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "!rm -rf data.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519107d",
   "metadata": {
    "papermill": {
     "duration": 0.018398,
     "end_time": "2024-05-18T17:54:49.711320",
     "exception": false,
     "start_time": "2024-05-18T17:54:49.692922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Keep Exploring! ðŸ‘€\n",
    "\n",
    "Thank you for delving into this notebook! If you found it insightful or beneficial, I encourage you to explore more of my projects and contributions on my profile.\n",
    "\n",
    "ðŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainalipk) ðŸ‘ˆ\n",
    "\n",
    "[GitHub]( https://github.com/zulqarnainalipk) |\n",
    "[LinkedIn]( https://www.linkedin.com/in/zulqarnainalipk/)\n",
    "\n",
    "## Share Your Thoughts! ðŸ™\n",
    "\n",
    "Your feedback is invaluable! Your insights and suggestions drive our ongoing improvement. If you have any comments, questions, or ideas to contribute, please feel free to reach out.\n",
    "\n",
    "ðŸ“¬ Contact me via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n",
    "\n",
    "I extend my sincere gratitude for your time and engagement. Your support inspires me to create even more valuable content.\n",
    "Happy coding and best of luck in your data science endeavors! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 27710,
     "sourceId": 33095,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 27711,
     "sourceId": 33096,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 269.10771,
   "end_time": "2024-05-18T17:54:50.650372",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T17:50:21.542662",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
